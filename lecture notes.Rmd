---
title: "MIMP Lectures"
output: html_notebook
---

# Session A

### Slide 20

$Y_{obs}$ should be 1 in $P(R|Y_{obs}, \psi)$: it only depends on a constant, like 2/7 in the example. So, $P(R|1,\psi=\frac{2}{7})$.

### Slide 23

The $\theta$'s are the variables of scientific interest. The missing data model is generally not. If the two models are independent, we say that the missingness is ignorable.

# Session C

### Slide 39

LOCF means last observation carried forward.

### Slides 46-56

- $Q$ is a quantity of scientific interest in the population.

- $\hat{Q}$ is the estimate that accounts for the sampling uncertainty

- $\hat{Q}_l$ is the estimate of the $l$-th repeated imputation (contains k parameters and is represented as a k × 1 column vector)

- $\bar{Q}$ is the pooled estimate and accounts for the sampling and missing data uncertainty; it's the average of all $\hat{Q}_l$s 

- $\bar{U}_l$ is the is the variance in the estimate, more specifically the variance-covariance matrix of $\hat{Q}$ obtained for the $l$-th imputation

- $\bar{U}$ is the average of the complete-data variances: $\frac{1}{m} \sum_{\ell=1}^{m} \bar{U}_{\ell}$. It's the variance caused by the fact that we are taking a sample
rather than the entire population. This is the conventional
statistical measure of variability.

- $B$ is the vVariance between the m complete-data estimates: $\frac{1}{m-1} \sum_{\ell=1}^{m}\left(\hat{Q}_{\ell}-\bar{Q}\right)\left(\hat{Q}_{\ell}-\bar{Q}\right)^{\prime}$. It's the extra variance caused by the fact that there are missing
values in the sample.

- $T$ is the total variance: $\bar{U}+B+\frac{B}{m}$ = 
$\bar{U}+\left(1+\frac{1}{m}\right) B$, where $\frac{B}{m}$ is called the simulation error: the extra simulation variance caused by the fact that $\bar{Q}$ itself is based on finite m

- $\lambda$ is the proportion of the variation attributable to the missing data: $\frac{B+\frac{B}{m}}{T}$. Interpret as "high" when $\lambda > .5$.

- $r$ is the relative increase in variance due to nonresponse: $\frac{B+\frac{B}{m}}{\overline{U}} = \frac{\lambda}{1-\lambda}$. 

- $\gamma$ is the fraction of information about Q missing due to nonresponse. It's similar to $\lambda$, but adjusted for the finite number of imputations. Interpret as "modest" up to 0.2, 0.3 as “moderately large” and 0.5 as “high”. You need the adjusted degrees of freedom to compute it, which is complicated. See https://stefvanbuuren.name/fimd/sec-whyandwhen.html.


# Session E

### Slide 104

Monotone imputation not often used, only with drop-out without replacement. JM is more generally used: joint modeling. Esitmate means and var-cov matrix and impute the data, estimate var-cov matrix again, impute again, ... (MCMC technique). But this technique is not very flexible and can be too strict (not reflecting all relations between variables that may be of interest in the complete-data analysis). 

In MICE, FCS is used: fully conditional specification. Create model for the first variable with missingness based on the values (real or imputed) of all other variables. Do this for each column left to right. This is one iteration. After 5-10 iterations, a solution is reached (no influence of the initial values anymore). The downside is that we do not have a theoretical distribution for this joint distribution (only for linear regressions we get a multivariate normal). Therefore we cannot use computartional shortcuts.

# Session G

### Slide 113

MICE is a kind of Gibbs sampler, so the rate of convergence is dependent on the correlations between iterations. The addition of noise (difference between predicted value (regression) and observed (pmm)) takes care of low correlations.

<!-- Initial values?? -->

### Slide 199

Incompatibility: Potential problem in the algorithm when we specify variables independently of each other. Then it can never form a multivariate distribution. Theoretical requirement for Gibbs sampler is that there is a known distribution to converge to. MICE appears to be quite robust, as long as you model each variable well from the other variables.

### Slide 125

Congeniality: Problem arising from relation between imputation model and complete-data model. The imputation model cannot be smaller (less flexible) than the complete-data model. However, "the more the better" does not always hold. See practical (session H).

### Slide 135

MAR/ignorability assumption: if we fit a model for the top cases (without missingness in Y), it is similar to the model on the bottom cases (with missings in Y) in the data set where the data are sorted based on missingness in Y (left lower corner) and all Xs are observed.

### Slide 137

Recipe for MICE:

- Include all variables that appear in the complete-data model, including transformations and interactions.

- In addition, include the variables that are related to the nonresponse.

- In addition, include variables that explain a considerable amount of variance (external information that is informative for the missing cells to get more precise results).

- Remove from the variables selected in steps 2 and 3 those variables that have too many missing values within the subgroup of incomplete cases (if the NAs of these values are in the same row as the NAs you're trying to impute).

### Slide 139

JAV: just another variable will give linear dependencies because we 'copy' the others into a transformation. Problem is that the ratio is imputed without the deterministic relation known.

Passive imputation is generally better because the relation is specified inside the imputations: use the tilde to do this. But aactually, none is recommended.

Substantive model methods (smcfsc) appear to be much better than others. Also called model-based imputation.

